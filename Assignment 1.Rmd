---
title: "Data science for economists - Assignment 1"
author: "Dan Boguslavsky"
date: "5/03/2020"
output:
  html_document:
    df_print: paged
---
Required packages installation:
```{r}
#install.packages('magrittr')
#install.packages("microbenchmark")
#install.packages('data.table')
#install.packages('ggplot2')
#install.packages("reshape2")
#install.packages('lubridate')
#install.packages('stringr')
#install.packages('chron')
#install.packages("ggplot2")
#install.packages("corrplot")
#install.packages("leaflet")
#install.packages("rnaturalearthdata")
#install.packages('dplyr')
```

```{r}
rm(list = ls())
```

#Question 1 
#a.
```{r}
my_aggregation <- function(x, is.truncated = FALSE){
  if(is.truncated){
    x <- x[x>=quantile(x,0.05)&x<=quantile(x,0.95)] #Discard 5th quantile and 95th quantile from vector x.
    return(list("mean" = mean(x), "var" = var(x), "med" = median(x)))
  }#close-if
  list("mean" = mean(x), "var" = var(x), "med" = median(x))
}#close-func-"my_aggregation"
```

#b.
We expect the aggregates to be very different when there is an extremely low or high value (or a few values) in the vector that is very different from most of the values.
If those "extreme" values are "balanced", meaning we have them on opposite sides, for example, a vector with a mean of 10 with most of the values in it, around its mean, and two values of 1,000,000 and (-1,000,000).
In this case, the mean should not be very much changed, although, the variance will decrease greatly.
On the other hand, if we have a vector that its variance is large enough because its values are "spreaded", but imbalanced, we are likely to see the variance decreasing slightly, but the mean changed radically.
The robustness of the median is based on the fact that we discard the values from both sides with the same percentage.

```{r}
set.seed(256)
dis<-rlnorm(1000000,1,0.5)
my_aggregation(dis)
my_aggregation(dis,is.truncated = TRUE)
```
We can see that the mean has not changed much, but the variance had decreased by half, this is because we eliminated the large and small values  on both sides somewhat eaqualy so the mean did not changed, but because the spead is smaller now, the variance is smaller.
Also, we can see, as expected, that the median did not changed.

#c.
```{r}
#Adjust the function to return the mean only:

my_aggregation_mean <- function(x, is.truncated = FALSE){
  if(is.truncated){
    x <- x[x>=quantile(x,0.05)&x<=quantile(x,0.95)]
    return(list("mean" = mean(x)))
  }#close-if
  list("mean" = mean(x))
}#close-func-"my_aggregation_mean"

#Now lets compare the run time:

library('microbenchmark')
microbenchmark(
my_aggregation_mean(dis, is.truncated = TRUE),
mean(dis,trim = 0.05),
times = 30 #Using 30 instead of default 100 just to save time.
)
```
We can see that the R base function is much faster (almost twice as much) than ours.
The reason is probably because R is using its own efficient algorithms to subset the data.

#Question 2

#a.
```{r}
aq<-airquality
apply(aq,2,FUN = function(x) sum(is.na(x)))
is.na_row <- apply(aq,1, FUN = function (x) anyNA(x))
```

#b.
```{r}
library('data.table')
  airquality_imputed <- as.data.table(aq)
  airquality_imputed <- 
    airquality_imputed[,lapply(.SD,FUN = function(x) ifelse(is.na(x),mean(x,na.rm = TRUE),as.double(x))),by = Month]
```

#c.
```{r}
plot(airquality_imputed$Ozone,airquality_imputed$Solar.R, xlab = "Ozone",ylab = "Solar.R")
imputed_points<-which(is.na(airquality$Ozone)&is.na(airquality$Solar.R))
points(airquality_imputed$Ozone[imputed_points],airquality_imputed$Solar.R[imputed_points],col="red",pch = 19)
legend("bottomright", legend = c("Fully-imputed","Original / Semi-imputed"),col=c("red", "black"), pch=c(19,1), cex=0.8)
```

#Question 3

```{r}
library('ggplot2')
data("diamonds")
```

#a.
```{r}
dim(diamonds)
class(diamonds)
```
The class is - data frame, made out of a "tbl" class. The dimensions are 53940 rows over 10 columns.

#b.
```{r}
library('magrittr')
library('data.table')
set.seed(256)
d <- diamonds[sample(nrow(diamonds),nrow(diamonds)*0.25),] %>% as.data.table()
```

#c.
```{r}
mosaicplot(~cut+color,data = d)
```
In this plot we can observe the distribution of the entire data, based on the color and cut variables.
For example, it is obvious that the "Ideal" cut is the most frequent one and the "Fair" cut is the less frequent one.
Now, we can also observe the distribution of the color in each cut level. For example - the "G" color seems to be the most frequent
in the "Ideal" cut group, but the "J" color - the less frequent one in that group.

#d.
```{r}
#1.
d[,"logp":=log10(d$price)]
#2.
d[,"v":=d$x*d$y*d$z]
#3.
median_depth<-median(d$depth)
d[,"cond1":=(((d$cut=="Ideal") + (d$depth<median_depth) + (d$clarity!="I1") + (d$color%in%c("D","E","F","G")))==3)]
```

#e.
```{r}
d[,list(Vmean = mean(v),Vvar = var(v), LOGPmean = mean(logp), LOGPvar = var(logp)),by = cond1]
```

#f.
```{r}
library('magrittr')
color_cut<-expand.grid(unique(d$cut),unique(d$color)) %>% as.data.table()
colnames(color_cut)[1]<-"cut"
colnames(color_cut)[2]<-"color"
set.seed(256)
color_cut[,"some_feature":=rnorm(nrow(color_cut))]
d<-merge(d,color_cut,by=c("cut","color"),all = TRUE)
#1.
d[,list(some_feature_mean = mean(some_feature)),by = "clarity"]
#2.
d[some_feature>1,list(PRICE_sd = sd(price), PRICE_iqr = IQR(price), PRICE_mad = mad(price)),by = "cut"]
#3. we are looking for those who make (cond1 or cond2) but not both.
d[((1<carat&2>carat)|(5000<price&10000>price))&(!((1<carat&2>carat)&(5000<price&10000>price))),.N,by="color"]
```

#g.
```{r}
library('reshape2')
acast(d,cut~color,value.var = "price",mean)
```

#Question 4
```{r}
library('ggplot2')
data("mpg")
mpg$manufacturer<-factor(mpg$manufacturer)
mpg$class<-factor(mpg$class)
mpg$year<-factor(mpg$year)
ggplot(mpg,aes(x=manufacturer,fill=manufacturer))+geom_histogram(stat = "count") + scale_x_discrete(labels = NULL)
ggplot(mpg,aes(x=displ,y=hwy,color=class))+geom_point(aes(shape = fl)) + facet_wrap(~manufacturer, ncol = 5)
ggplot(mpg,aes(x=displ,y=hwy,color=cyl)) + geom_point() +
          geom_smooth(span=0.7,colour = "blue",level=0.95) + geom_smooth(span=0.3,colour = "red",level=0.95)
ggplot(mpg,aes(x=displ)) + geom_density(aes(group = hwy>23,fill = hwy>23),alpha=0.5) +
          facet_wrap(vars(year)) +
          theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),panel.background = element_blank(),
          legend.position="bottom") 
```
#Question 5

#a.
```{r}
airq<-airquality %>% as.data.table()
stock<-EuStockMarkets %>% as.data.table()
```

#b.
```{r}
library('data.table')
airq[,"date":=as.Date(paste(airq$Day,airq$Month,"2019",sep = "-"),format = c("%d-%m-%Y"))]
stock[,"date":=seq.Date(from = as.Date("2019-01-01",format = c("%Y-%m-%d")),by = "day",length.out = nrow(stock))]
```

#c.
```{r}
setkey(airq,"date")
setkey(stock,"date")
stock<-stock[airq,,]
```

#d.
```{r}
library('lubridate')
difference<-stock$date[stock$CAC==(min(stock$CAC))] %--%stock$date[stock$CAC==(max(stock$CAC))] 
#The time period is:
as.period(difference, unit = 'month') #in months
as.period(difference, unit = 'day') #in days
as.period(difference, unit = 'hour') #in hours
```

#e.
```{r}
library('magrittr')
stock$Temp[stock$date %between% c(ymd(int_start(difference)),ymd(int_end(difference)))] %>% mean()
```

#f.
```{r}
library('ggplot2')

ggplot(data = stock,aes(x=date)) + geom_line(aes(date,CAC)) + geom_line(aes(date,DAX)) + geom_line(aes(date,SMI)) +                         geom_line(aes(date,FTSE)) + ylab("price") +
      geom_vline(xintercept = as.numeric(stock[stock$Solar.R == min(stock$Solar.R,na.rm = T),"date"]))
```

#g.
```{r}
stock[,"week_num":=week(stock$date)]
stock[,list(CAC_mean = mean(CAC), TEMP_mean = mean(Temp)),by = "week_num"]
##if only the MIN-MAX intervanl needed:
stock[stock$date %between% c(ymd(int_start(difference)),ymd(int_end(difference))),
      list(CAC_mean = mean(CAC), TEMP_mean = mean(Temp)),by = "week_num"]
```


#Question 6

```{r}
WHR2017<-read.csv("/Users/danboguslavsky/git/datascience/2017.csv")
```

#a.
```{r}
#colnames(WHR2017)
for (i  in 1:NROW(colnames(WHR2017))){
  colnames(WHR2017)[i]<-gsub("..","-",colnames(WHR2017)[i],fixed = TRUE)
}#for_loop
```

#b.
```{r}
attach(WHR2017)
to_plot<-list(`Economy-GDP.per.Capita.`,`Family`,`Health-Life.Expectancy.`,`Freedom`)
par(mfrow = c(2,2))
for(plot_var in to_plot){
  plot(x=`Happiness.Score`,y=plot_var)
  graphics::text(`Happiness.Score`,plot_var,Country)
  graphics::text(`Happiness.Score`,plot_var,ifelse(Country=='Israel','Israel',""),font = 2, col="blue")
}#Close_for_loop
#graphics::text(Country)
detach(WHR2017)
```

#c.
```{r}
library('corrplot')
WHR2017_numerics<-WHR2017[,-c(1)]
corrplot(cor(WHR2017_numerics),order = "AOE")
```
We can see that the "Whisker.high","Whisker.low" and "Happiness.Score" are very highly and positivly correlated with each-other.
"Happiness.Rank" is also highly correlated with the previous three but negativly.
This makes very much sense as high "Happiness.Score" indicates lower rank - meaning higher position. 
"Economy-GDP.per.Capita.","Health-Life.Expectancy" and "Family" are also fairly correlated with the other four.

#d.Bonus.
```{r}
#library("rnaturalearthdata")
library('leaflet')
library('magrittr')
contries_lng_lat<-data.frame(NA,NA,NA)
countries_data<-rnaturalearthdata::map_units110
country_num <- which(WHR2017$Country %in% countries_data$name_long)
for (i in country_num){
  new<-data.frame(NA,NA,NA)
  new[1,1]<-countries_data$name_long[i]
  new[1,2]<-countries_data@polygons[[i]]@labpt[1]
  new[1,3]<-countries_data@polygons[[i]]@labpt[2]
  contries_lng_lat<-rbind(contries_lng_lat,new)
}#Close_for
colnames(contries_lng_lat)<-c("Country","lng","lat")
contries_lng_lat<-contries_lng_lat[-1,]
contries_lng_lat<-merge(contries_lng_lat,subset(WHR2017,select=c("Country","Happiness.Score")),by = "Country",all.x = T)
contries_lng_lat$Name_Score<-paste(contries_lng_lat$Country," - "," Score: ",round(contries_lng_lat$Happiness.Score,3),sep = " ")
leaflet() %>% addTiles() %>% addMarkers(lng = contries_lng_lat$lng,lat = contries_lng_lat$lat, label = contries_lng_lat$Name_Score)
```

#Question 7

```{r}
library('data.table')
autos<-fread(file = "/Users/danboguslavsky/git/datascience/autos.csv", encoding = "Latin-1")
```

#a.
```{r}
library('magrittr')
grep("Mazda",autos$name,ignore.case = TRUE) %>% length()
mazda <- autos[grep("Mazda",autos$name,ignore.case = TRUE),] %>% as.data.table()
```

#b.
```{r}
mazda[,"is_3":= grepl("3",mazda$name)]
```

#c.
```{r}
library('lubridate')
mazda[,list(Created_to_Seen_time =mean(difftime(as.POSIXct(lastSeen),as.POSIXct(dateCreated),units = "hours")),
            Num_of_obs = .N, Diesel_sahre = (sum(fuelType == "diesel"))/.N),by = "is_3"]
```


#Question 8
#a.
```{r}
zeros<-function(d){
  a<-matrix(0,d,d)
  a[c(1,d),] <-1
  a[,c(1,d)] <-1
  return(a)
}#close_function_"zeros"
```

#b.
```{r}
same<-function(a,b){
  if(length(a)==length(b)){
    for (i in 1:length(a)) {
      if(a[i]!=b[i]){return(FALSE)}
    }#close_for
    return(TRUE) #no non identical values found -> the vectors are identical
  }#close_if
  return(FALSE) #not the same length -> not identical
}#close_function
```

#c.
```{r}
library('stringr')
counter<-function(a,b){
 count = 0
 a<-str_split(a,"",simplify = T)
 for (char in a){
   if (char==b){count = count + 1}
 }#close for loop
 return(count)
}#close function
```

#d. 
```{r}
birthday <- function(birthday){
  birthday<-as.Date(birthday)
  print(weekdays(birthday))
  difference<-(Sys.Date()-birthday)
  print(difference)
  print(paste("Next birthday in: ",(ceiling(difference/365)-(difference/365))*365," days",sep = ""))
}
```

#Question 9
```{r}
library('ggplot2')
data("diamonds")
```

#a.
```{r}
numeric_diamonds <- unlist(lapply(diamonds, is.numeric)) 
numeric_diamonds<-diamonds[,numeric_diamonds]
cor_mat<-matrix(NA,ncol(numeric_diamonds),ncol(numeric_diamonds))
for(i in 1:ncol(numeric_diamonds)){
  for(j in 1:ncol(numeric_diamonds)){
    cor_mat[i,j]<-(cor(numeric_diamonds[,i],numeric_diamonds[,j]))
  }#close_j_loop
}#close_i_loop
colnames(cor_mat)<-names(numeric_diamonds)
rownames(cor_mat)<-names(numeric_diamonds)
cor_mat
```
The importance of a correlation matrix in the context of date science is being expressed as it lets us see pattern between a large amount of variables. Thanks to it, we can show a very important information is a very simple and basic form.

#b.
No, we cannot compute the Pearson correlation between 'cut' and 'color' as they are both "categorial" variables and cannot be used in Pearson's correlation formula.

#c.
```{r}
library('magrittr')
library('dplyr')
library('data.table')
cut_by_color<-diamonds %>% group_by(cut, color) %>% summarise(n=n()) %>% dcast(color~cut) 
cut_by_color<-as.data.table(cut_by_color)
cut_by_color[,.SD/sum(.SD),by = "color"]
```

#d.
```{r}
diamonds$color<-as.integer(diamonds$color)
cor(diamonds$color,diamonds$carat)
```
This value does not have any meaning.
A color can not be presented as a number value which will mean anything except a category.

#e.
We can present a the carat to color relationship with a Boxplot. We can see for each color its carat specifications:
```{r}
library('data.table')
data("diamonds") 
boxplot(diamonds$carat~diamonds$color,data=diamonds, main="Color vs Carat", xlab="Color", ylab="Carat")
```
So we can see here that as we go from D to J the median of carat value is increasing.

#Question 10:
#a.
```{r}
MAD_comp <- function(x){
  vec_x <- sort(x)
  x_median <- median(x)
  deviations<-c()
  for (i in vec_x){
    deviations<-c(deviations, abs(i-x_median))
  }#close_for
  deviations<-sort(deviations)
  return(median(deviations)*1.4826)
}#close_function
```

#b.
```{r}
set.seed(256)
vec_10_norm <- rnorm(10,mean = 1 , sd = 1)
sd(vec_10_norm)
MAD_comp(vec_10_norm)
```

#c.
```{r}
set.seed(256)
vec_10_exp<- rexp(10,rate = 1)
sd(vec_10_exp)
MAD_comp(vec_10_exp)
```

#d.
We would expect the 'MAD' to be closer when using with normal distrebution  and the 'sd' to be more apart. This is because normal's distrebution Median and mean are close to each other.
In the exponential distrebution, the mean is shifted but the median stays aproximatly the same.
```{r}
paste("Difference in standard diviation: ", sd(vec_10_exp)-sd(vec_10_norm), " (Exponentian - Normal)",sep = "")
paste("Difference in MAD: ", MAD_comp(vec_10_exp)- MAD_comp(vec_10_norm), " (Exponentian - Normal)",sep = "")
```
We can see that both results are greather within the Exponentian distrebution but the the 'sd' difference is much larger.

#e.
```{r}
norm_diff_vec<-c()
exp_dif_vec<-c()
for(i in 1:1000){
  norm_diff_vec<-c()
  exp_dif_vec<-c()
  set.seed(256)
  vec_10_norm <- rnorm(10,mean = 1 , sd = 1)
  set.seed(256)
  vec_10_exp<- rexp(10,rate = 1)
  norm_difference <- abs(MAD_comp(vec_10_norm) - sd(vec_10_norm))
  exp_difference <- abs(MAD_comp(vec_10_exp) - sd(vec_10_exp))
  norm_diff_vec<-c(norm_diff_vec,norm_difference)
  exp_dif_vec<-c(exp_dif_vec,exp_difference)
}
mean(norm_diff_vec)
mean(exp_dif_vec)
```

#f.
In clause 'd' as explained, due to the robustness of the Median, the difference in the MAD is much smaller the the difference in the Standard Deviation. Even though the tai is pulling the mean in the exponential distrebution, the median stays aproximatly the same.

In clause 'e' we can see that the average difference of the Exponential distrebution is greather, because in both distrebutions, the MAD is aproximatly the same but the standard deviation is greathet in the exponential distrebution, so the avarage difference in larger being calculated on the exponential distrebution.